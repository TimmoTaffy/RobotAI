#!/usr/bin/env python3
"""
å¯¼èˆªæ§åˆ¶æ¨¡å— - ç»Ÿä¸€æµ‹è¯•å…¥å£

è¿™æ˜¯é¡¹ç›®çš„ä¸»è¦æµ‹è¯•å·¥å…·ï¼Œç»Ÿä¸€äº†æ‰€æœ‰æµ‹è¯•å‘½ä»¤ã€‚å»ºè®®ä¼˜å…ˆä½¿ç”¨æ­¤è„šæœ¬è€Œéç›´æ¥è°ƒç”¨pytestã€‚

å¿«é€Ÿå¼€å§‹ï¼š
    python run_tests.py --help          # æŸ¥çœ‹å®Œæ•´å¸®åŠ©
    python run_tests.py --quick         # å¿«é€ŸéªŒè¯ï¼ˆæ¨èæ—¥å¸¸ä½¿ç”¨ï¼‰
    python run_tests.py --all           # å®Œæ•´æµ‹è¯•+æŠ¥å‘Šï¼ˆæ¨èæäº¤å‰ï¼‰
    
æŠ¥å‘Šä½ç½®ï¼š
    â€¢ JSONæŠ¥å‘Šï¼šreports/test-results.json
    â€¢ HTMLæŠ¥å‘Šï¼šreports/test-report.html  
    â€¢ è¦†ç›–ç‡ï¼šhtmlcov/index.html
    
é«˜çº§è°ƒè¯•è¯·ç›´æ¥ä½¿ç”¨ pytest å‘½ä»¤ï¼ˆè§æ–‡æ¡£ 4.2 çš„5.2èŠ‚ï¼‰
"""

import argparse
import subprocess
import sys
import os
import time
from pathlib import Path

def run_command(cmd, description, timeout=300):
    """è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºç»“æœ"""
    print(f"\n{'='*60}")
    print(f"ğŸƒ {description}")
    print(f"{'='*60}")
    print(f"å‘½ä»¤: {' '.join(cmd)}")
    print("-" * 60)
    
    start_time = time.time()
    try:
        result = subprocess.run(
            cmd, 
            timeout=timeout,
            capture_output=False,  # å®æ—¶æ˜¾ç¤ºè¾“å‡º
            text=True
        )
        
        duration = time.time() - start_time
        
        if result.returncode == 0:
            print(f"\nâœ… {description} æˆåŠŸå®Œæˆ! (è€—æ—¶: {duration:.1f}s)")
        else:
            print(f"\nâŒ {description} å¤±è´¥! (è€—æ—¶: {duration:.1f}s)")
            
        return result.returncode == 0
        
    except subprocess.TimeoutExpired:
        print(f"\nâ° {description} è¶…æ—¶ (>{timeout}s)")
        return False
    except Exception as e:
        print(f"\nğŸ’¥ {description} å‡ºé”™: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(
        description="ğŸ¤– å¯¼èˆªæ§åˆ¶æ¨¡å—ç»Ÿä¸€æµ‹è¯•å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ¨èä½¿ç”¨æ–¹å¼:
  python run_tests.py --quick            # æ—¥å¸¸å¿«é€ŸéªŒè¯
  python run_tests.py --all              # æäº¤å‰å®Œæ•´éªŒè¯ï¼ˆç”Ÿæˆæ‰€æœ‰æŠ¥å‘Šï¼‰
  python run_tests.py --unit --coverage  # å•å…ƒæµ‹è¯•+è¦†ç›–ç‡åˆ†æ
  python run_tests.py --module planning  # è°ƒè¯•ç‰¹å®šæ¨¡å—

vs pyteståŸç”Ÿå‘½ä»¤:
  æœ¬è„šæœ¬ = pytest + è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆ + å‹å¥½ç•Œé¢
  ç›´æ¥ä½¿ç”¨pytestè¯·å‚è€ƒæ–‡æ¡£ã€Š4.2 è¿è°ƒï¼šå¯¼èˆªä¸æ§åˆ¶ã€‹ç¬¬5.2èŠ‚
        """
    )
    
    parser.add_argument('--all', action='store_true', help='è¿è¡Œæ‰€æœ‰æµ‹è¯•')
    parser.add_argument('--unit', action='store_true', help='è¿è¡Œå•å…ƒæµ‹è¯•')
    parser.add_argument('--integration', action='store_true', help='è¿è¡Œé›†æˆæµ‹è¯•')
    parser.add_argument('--performance', action='store_true', help='è¿è¡Œæ€§èƒ½æµ‹è¯•')
    parser.add_argument('--robustness', action='store_true', help='è¿è¡Œé²æ£’æ€§æµ‹è¯•')
    parser.add_argument('--coverage', action='store_true', help='ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæµ‹è¯•ï¼ˆè·³è¿‡æ…¢é€Ÿæµ‹è¯•ï¼‰')
    parser.add_argument('--module', type=str, help='æµ‹è¯•ç‰¹å®šæ¨¡å— (path_planner, motion_planner, etc.)')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--parallel', '-j', type=int, help='å¹¶è¡Œæµ‹è¯•è¿›ç¨‹æ•°')
    
    args = parser.parse_args()
    
    # ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•
    project_root = Path(__file__).parent
    os.chdir(project_root)
    
    print("ğŸ¤– å¯¼èˆªæ§åˆ¶æ¨¡å—æµ‹è¯•å·¥å…·")
    print(f"ğŸ“ å·¥ä½œç›®å½•: {project_root}")
    print(f"ğŸ Pythonç‰ˆæœ¬: {sys.version}")
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not Path("tests").exists():
        print("âŒ æœªæ‰¾åˆ°testsç›®å½•!")
        return 1
    
    if not Path("requirements.txt").exists():
        print("âŒ æœªæ‰¾åˆ°requirements.txt!")
        return 1
    
    # æ„å»ºpytestå‘½ä»¤
    pytest_cmd = [sys.executable, "-m", "pytest"]
    
    # é»˜è®¤å‚æ•°
    if args.verbose:
        pytest_cmd.append("-v")
    else:
        pytest_cmd.append("-q")
    
    # å¹¶è¡Œæµ‹è¯•
    if args.parallel:
        pytest_cmd.extend(["-n", str(args.parallel)])
    
    # è¦†ç›–ç‡
    if args.coverage or args.all:
        pytest_cmd.extend([
            "--cov=.", 
            "--cov-report=term-missing",
            "--cov-report=html:htmlcov"
        ])
    
    # ç”ŸæˆæŠ¥å‘Š
    if args.all:
        pytest_cmd.extend([
            "--html=reports/test-report.html",
            "--self-contained-html",
            "--json-report",
            "--json-report-file=reports/test-results.json"
        ])
        # ç¡®ä¿reportsç›®å½•å­˜åœ¨
        Path("reports").mkdir(exist_ok=True)
    
    success_count = 0
    total_tests = 0
    
    # æ ¹æ®å‚æ•°é€‰æ‹©æµ‹è¯•
    if args.all:
        total_tests += 1
        if run_command(pytest_cmd + ["tests/"], "æ‰€æœ‰æµ‹è¯•", 600):
            success_count += 1
            
    elif args.module:
        total_tests += 1
        test_file = f"tests/test_{args.module}.py"
        if Path(test_file).exists():
            if run_command(pytest_cmd + [test_file], f"{args.module}æ¨¡å—æµ‹è¯•"):
                success_count += 1
        else:
            print(f"âŒ æœªæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶: {test_file}")
            
    else:
        # åˆ†ç±»è¿è¡Œæµ‹è¯•
        if args.unit:
            total_tests += 1
            cmd = pytest_cmd + ["-m", "unit", "tests/"]
            if args.quick:
                cmd.extend(["-m", "not slow"])
            if run_command(cmd, "å•å…ƒæµ‹è¯•"):
                success_count += 1
        
        if args.integration:
            total_tests += 1
            if run_command(pytest_cmd + ["-m", "integration", "tests/"], "é›†æˆæµ‹è¯•"):
                success_count += 1
        
        if args.performance:
            total_tests += 1
            if run_command(pytest_cmd + ["-m", "performance", "tests/"], "æ€§èƒ½æµ‹è¯•"):
                success_count += 1
        
        if args.robustness:
            total_tests += 1
            if run_command(pytest_cmd + ["-m", "robustness", "tests/"], "é²æ£’æ€§æµ‹è¯•"):
                success_count += 1
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•æµ‹è¯•ç±»å‹ï¼Œè¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•
        if not any([args.unit, args.integration, args.performance, args.robustness]):
            total_tests += 1
            cmd = pytest_cmd + ["tests/"]
            if args.quick:
                cmd.extend(["-m", "not slow"])
            if run_command(cmd, "é»˜è®¤æµ‹è¯•ï¼ˆæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼‰"):
                success_count += 1
    
    # æ€»ç»“
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æµ‹è¯•æ€»ç»“")
    print(f"{'='*60}")
    print(f"æˆåŠŸ: {success_count}/{total_tests}")
    
    if success_count == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼")
        if args.coverage or args.all:
            print("ğŸ“‹ è¦†ç›–ç‡æŠ¥å‘Š: htmlcov/index.html")
        if args.all:
            print("ğŸ“„ è¯¦ç»†æŠ¥å‘Š: reports/test-results.json")
            print("ğŸŒ å¯è§†åŒ–æŠ¥å‘Š: reports/test-report.html")
        print("\nğŸ’¡ æç¤º: å¦‚éœ€é«˜çº§è°ƒè¯•ï¼Œè¯·å‚è€ƒæ–‡æ¡£ã€Š4.2 è¿è°ƒï¼šå¯¼èˆªä¸æ§åˆ¶ã€‹ç¬¬5.2èŠ‚ä½¿ç”¨pytestå‘½ä»¤")
        return 0
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šé¢çš„è¾“å‡º")
        print("ğŸ”§ è°ƒè¯•å»ºè®®: ä½¿ç”¨ pytest tests/test_xxx.py::test_func -v -s ç²¾ç¡®å®šä½é—®é¢˜")
        return 1

if __name__ == "__main__":
    sys.exit(main())
