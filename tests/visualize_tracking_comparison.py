"""
å¢å¼ºè·Ÿè¸ªå™¨é…ç½®å¯¹æ¯”å¯è§†åŒ–ï¼šå±•ç¤ºæ ‡å‡†é…ç½® vs é«˜ç²¾åº¦é…ç½®çš„æ•ˆæœ
"""
import matplotlib.pyplot as plt
import numpy as np
import time
from typing import List, Tuple
from unittest.mock import Mock

from src.tracking.enhanced_tracker import EnhancedTargetTracker
from src.tracking import create_production_tracker, create_high_precision_tracker
from src.common.types import VisionRobot


class TrackingScenarioGenerator:
    """è·Ÿè¸ªåœºæ™¯ç”Ÿæˆå™¨"""
    
    def __init__(self, dt: float = 0.1):
        self.dt = dt
    
    def generate_moving_targets(self, scenario: str, duration: float = 10.0) -> List[List]:
        """ç”Ÿæˆç§»åŠ¨ç›®æ ‡æ£€æµ‹åºåˆ—"""
        steps = int(duration / self.dt)
        detection_sequence = []
        
        if scenario == "single_moving":
            # å•ä¸ªç›®æ ‡ç›´çº¿è¿åŠ¨
            for step in range(steps):
                t = step * self.dt
                detection = self._create_vision_robot(
                    x=2.0 + t * 0.5,  # 0.5 m/s
                    y=3.0 + t * 0.3,  # 0.3 m/s  
                    team='enemy',
                    conf=0.9 + 0.1 * np.sin(t)  # ç½®ä¿¡åº¦è½»å¾®æ³¢åŠ¨
                )
                detection_sequence.append([detection])
                
        elif scenario == "multiple_crossing":
            # å¤šç›®æ ‡äº¤å‰è¿åŠ¨
            for step in range(steps):
                t = step * self.dt
                detections = []
                
                # ç›®æ ‡1ï¼šä»å·¦åˆ°å³
                if t < 8.0:  # åœ¨è§†é‡å†…8ç§’
                    det1 = self._create_vision_robot(
                        x=1.0 + t * 0.6,
                        y=5.0,
                        team='enemy',
                        conf=0.85 + 0.15 * np.sin(t * 2)
                    )
                    detections.append(det1)
                
                # ç›®æ ‡2ï¼šä»ä¸‹åˆ°ä¸Š
                if 2.0 < t < 9.0:
                    det2 = self._create_vision_robot(
                        x=6.0 + 0.2 * np.sin(t),  # ç•¥å¾®æ‘†åŠ¨
                        y=1.0 + (t - 2.0) * 0.8,
                        team='enemy', 
                        conf=0.8 + 0.2 * np.sin(t * 1.5)
                    )
                    detections.append(det2)
                
                # ç›®æ ‡3ï¼šç›Ÿå‹ï¼Œç›¸å¯¹é™æ­¢
                if t > 1.0:
                    det3 = self._create_vision_robot(
                        x=8.0 + 0.1 * np.sin(t * 0.5),
                        y=8.0 + 0.1 * np.cos(t * 0.5),
                        team='ally',
                        conf=1.0
                    )
                    detections.append(det3)
                
                detection_sequence.append(detections)
        
        return detection_sequence
    
    def _create_vision_robot(self, x: float, y: float, team: str, conf: float) -> VisionRobot:
        """åˆ›å»ºè§†è§‰æ£€æµ‹æœºå™¨äºº"""
        detection = VisionRobot(id=1, team=team, x=x, y=y)
        detection.confidence = conf
        return detection


class TrackingComparison:
    """è·Ÿè¸ªå™¨é…ç½®å¯¹æ¯”åˆ†æ"""
    
    def __init__(self, dt: float = 0.1):
        self.dt = dt
        self.scenario_gen = TrackingScenarioGenerator(dt)
    
    def run_comparison(self, scenario: str = "multiple_crossing", duration: float = 10.0):
        """è¿è¡Œè·Ÿè¸ªå™¨å¯¹æ¯”æµ‹è¯•"""
        print(f"ğŸ¯ Starting tracker configuration comparison test: {scenario}")
        
        # ç”Ÿæˆæµ‹è¯•åœºæ™¯
        detections_sequence = self.scenario_gen.generate_moving_targets(scenario, duration)
        
        # åˆ›å»ºä¸åŒé…ç½®çš„è·Ÿè¸ªå™¨
        standard_tracker = create_production_tracker(dt=self.dt)
        precision_tracker = create_high_precision_tracker(dt=self.dt)
        
        # è·Ÿè¸ªç»“æœè®°å½•
        standard_results = []
        precision_results = []
        compute_times = {'standard': [], 'precision': []}
        
        # æ¨¡æ‹Ÿå·±æ–¹ä½ç½®ï¼ˆç”¨äºå¨èƒè¯„ä¼°ï¼‰
        my_position = (0.0, 0.0)
        
        print("  ğŸ“Š Processing detections...")
        
        # å¤„ç†æ¯ä¸€å¸§æ£€æµ‹
        for step, detections in enumerate(detections_sequence):
            if not detections:
                continue
                
            # è½¬æ¢ä¸ºè·Ÿè¸ªå™¨æ‰€éœ€æ ¼å¼
            vision_data = Mock()
            vision_data.robots = detections
            
            radar_data = Mock()
            radar_data.robots = []  # è¿™ä¸ªåœºæ™¯åªç”¨è§†è§‰æ•°æ®
            
            # æ ‡å‡†é…ç½®è·Ÿè¸ªå™¨
            start_time = time.time()
            standard_targets = standard_tracker.process_detections(
                radar_data, vision_data, my_position
            )
            standard_time = time.time() - start_time
            compute_times['standard'].append(standard_time)
            
            # é«˜ç²¾åº¦é…ç½®è·Ÿè¸ªå™¨
            start_time = time.time()
            precision_targets = precision_tracker.process_detections(
                radar_data, vision_data, my_position
            )
            precision_time = time.time() - start_time
            compute_times['precision'].append(precision_time)
            
            # è®°å½•ç»“æœ
            standard_results.append({
                'time': step * self.dt,
                'targets': [(t.state[0], t.state[1], t.team, t.confidence, t.threat_level) 
                           for t in standard_targets],
                'count': len(standard_targets)
            })
            
            precision_results.append({
                'time': step * self.dt,
                'targets': [(t.state[0], t.state[1], t.team, t.confidence, t.threat_level) 
                           for t in precision_targets],
                'count': len(precision_targets)
            })
        
        # æ€§èƒ½åˆ†æ
        performance_stats = self._analyze_performance(
            standard_results, precision_results, compute_times, detections_sequence
        )
        
        # å¯è§†åŒ–ç»“æœ
        self._plot_comparison(
            standard_results, precision_results, performance_stats, 
            scenario, detections_sequence
        )
        
        return performance_stats
    
    def _analyze_performance(self, standard_results, precision_results, 
                           compute_times, detections_sequence):
        """åˆ†æè·Ÿè¸ªæ€§èƒ½"""
        
        # è®¡ç®—è·Ÿè¸ªè¿ç»­æ€§
        def calculate_track_continuity(results):
            if not results:
                return 0
            track_lifetimes = {}
            for frame in results:
                active_ids = set()
                for target in frame['targets']:
                    target_id = f"{target[2]}_{int(target[0]//2)}_{int(target[1]//2)}"
                    active_ids.add(target_id)
                    
                    if target_id not in track_lifetimes:
                        track_lifetimes[target_id] = 0
                    track_lifetimes[target_id] += 1
            
            return np.mean(list(track_lifetimes.values())) if track_lifetimes else 0
        
        # è®¡ç®—æ£€æµ‹è¦†ç›–ç‡
        def calculate_detection_coverage(results, detections):
            total_detections = sum(len(frame) for frame in detections)
            total_tracks = sum(r['count'] for r in results)
            return total_tracks / total_detections if total_detections > 0 else 0
        
        stats = {
            'standard_tracker': {
                'avg_targets': np.mean([r['count'] for r in standard_results]),
                'track_continuity': calculate_track_continuity(standard_results),
                'detection_coverage': calculate_detection_coverage(standard_results, detections_sequence),
                'avg_compute_time': np.mean(compute_times['standard']) * 1000,  # ms
                'max_compute_time': np.max(compute_times['standard']) * 1000
            },
            'precision_tracker': {
                'avg_targets': np.mean([r['count'] for r in precision_results]),
                'track_continuity': calculate_track_continuity(precision_results),
                'detection_coverage': calculate_detection_coverage(precision_results, detections_sequence),
                'avg_compute_time': np.mean(compute_times['precision']) * 1000,  # ms
                'max_compute_time': np.max(compute_times['precision']) * 1000
            }
        }
        
        return stats
    
    def _plot_comparison(self, standard_results, precision_results, stats, 
                        scenario, detections_sequence):
        """ç»˜åˆ¶å¯¹æ¯”ç»“æœ"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. è½¨è¿¹è·Ÿè¸ªå¯¹æ¯”
        ax = axes[0, 0]
        
        # ç»˜åˆ¶æ ‡å‡†é…ç½®è½¨è¿¹
        for frame in standard_results:
            for target in frame['targets']:
                color = 'red' if target[2] == 'enemy' else 'blue'
                ax.scatter(target[0], target[1], c=color, alpha=0.6, s=20, marker='o')
        
        # ç»˜åˆ¶é«˜ç²¾åº¦é…ç½®è½¨è¿¹
        for frame in precision_results:
            for target in frame['targets']:
                color = 'darkred' if target[2] == 'enemy' else 'darkblue'
                ax.scatter(target[0], target[1], c=color, alpha=0.8, s=30, marker='x')
        
        ax.set_title('Trajectory Tracking Comparison\\n(Circles: Standard, X-marks: High Precision)')
        ax.set_xlabel('X (m)')
        ax.set_ylabel('Y (m)')
        ax.grid(True, alpha=0.3)
        ax.legend(['Standard-Enemy', 'Standard-Ally', 'Precision-Enemy', 'Precision-Ally'])
        
        # 2. ç›®æ ‡æ•°é‡éšæ—¶é—´å˜åŒ–
        ax = axes[0, 1]
        times = [r['time'] for r in standard_results]
        standard_counts = [r['count'] for r in standard_results]
        precision_counts = [r['count'] for r in precision_results]
        
        ax.plot(times, standard_counts, 'b-', label='Standard Config', linewidth=2)
        ax.plot(times, precision_counts, 'r--', label='High Precision Config', linewidth=2)
        ax.set_title('Tracked Target Count')
        ax.set_xlabel('Time (s)')
        ax.set_ylabel('Target Count')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 3. ç½®ä¿¡åº¦åˆ†å¸ƒ
        ax = axes[0, 2]
        standard_confidences = []
        precision_confidences = []
        
        for frame in standard_results:
            for target in frame['targets']:
                standard_confidences.append(target[3])
                
        for frame in precision_results:
            for target in frame['targets']:
                precision_confidences.append(target[3])
        
        ax.hist(standard_confidences, bins=20, alpha=0.7, label='Standard Config', density=True)
        ax.hist(precision_confidences, bins=20, alpha=0.7, label='High Precision Config', density=True)
        ax.set_title('Target Confidence Distribution')
        ax.set_xlabel('Confidence')
        ax.set_ylabel('Density')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 4. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
        ax = axes[1, 0]
        metrics = ['Avg Targets', 'Track Continuity', 'Detection Coverage']
        standard_values = [
            stats['standard_tracker']['avg_targets'],
            stats['standard_tracker']['track_continuity'],
            stats['standard_tracker']['detection_coverage']
        ]
        precision_values = [
            stats['precision_tracker']['avg_targets'],
            stats['precision_tracker']['track_continuity'],
            stats['precision_tracker']['detection_coverage']
        ]
        
        x = np.arange(len(metrics))
        width = 0.35
        
        ax.bar(x - width/2, standard_values, width, label='Standard Config', color='skyblue')
        ax.bar(x + width/2, precision_values, width, label='High Precision Config', color='lightcoral')
        
        ax.set_title('Performance Metrics Comparison')
        ax.set_ylabel('Metric Value')
        ax.set_xticks(x)
        ax.set_xticklabels(metrics)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 5. è®¡ç®—æ—¶é—´å¯¹æ¯”
        ax = axes[1, 1]
        time_metrics = ['Avg Time', 'Max Time']
        standard_times = [
            stats['standard_tracker']['avg_compute_time'],
            stats['standard_tracker']['max_compute_time']
        ]
        precision_times = [
            stats['precision_tracker']['avg_compute_time'],
            stats['precision_tracker']['max_compute_time']
        ]
        
        x = np.arange(len(time_metrics))
        ax.bar(x - width/2, standard_times, width, label='Standard Config', color='lightgreen')
        ax.bar(x + width/2, precision_times, width, label='High Precision Config', color='orange')
        
        ax.set_title('Computation Time Comparison')
        ax.set_ylabel('Time (ms)')
        ax.set_xticks(x)
        ax.set_xticklabels(time_metrics)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 6. å¨èƒç­‰çº§çƒ­å›¾
        ax = axes[1, 2]
        threat_grid = np.zeros((10, 10))
        
        for frame in precision_results:  # ä½¿ç”¨é«˜ç²¾åº¦ç»“æœ
            for target in frame['targets']:
                if target[2] == 'enemy':  # åªçœ‹æ•Œæ–¹ç›®æ ‡
                    grid_x = min(int(target[0]), 9)
                    grid_y = min(int(target[1]), 9)
                    if 0 <= grid_x < 10 and 0 <= grid_y < 10:
                        threat_grid[grid_y, grid_x] += target[4]  # å¨èƒç­‰çº§
        
        im = ax.imshow(threat_grid, cmap='Reds', origin='lower')
        ax.set_title('Threat Level Heatmap (High Precision)')
        ax.set_xlabel('X Grid')
        ax.set_ylabel('Y Grid')
        plt.colorbar(im, ax=ax)
        
        plt.tight_layout()
        plt.savefig('tracking_configuration_comparison.png', dpi=300, bbox_inches='tight')
        print("  ğŸ“ˆ Comparison chart saved: tracking_configuration_comparison.png")
        
        # æ‰“å°æ€§èƒ½æ€»ç»“
        print("\\nğŸ“Š è·Ÿè¸ªå™¨é…ç½®å¯¹æ¯”ç»“æœ:")
        print("=" * 60)
        
        for tracker_name, tracker_stats in stats.items():
            print(f"\\n{tracker_name.replace('_', ' ').title()}:")
            print(f"  å¹³å‡ç›®æ ‡æ•°: {tracker_stats['avg_targets']:.1f}")
            print(f"  è·Ÿè¸ªè¿ç»­æ€§: {tracker_stats['track_continuity']:.1f}")
            print(f"  æ£€æµ‹è¦†ç›–ç‡: {tracker_stats['detection_coverage']:.2%}")
            print(f"  å¹³å‡è®¡ç®—æ—¶é—´: {tracker_stats['avg_compute_time']:.2f} ms")
            print(f"  æœ€å¤§è®¡ç®—æ—¶é—´: {tracker_stats['max_compute_time']:.2f} ms")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¢å¼ºè·Ÿè¸ªå™¨é…ç½®å¯¹æ¯”åˆ†æ")
    print("=" * 50)
    
    comparison = TrackingComparison(dt=0.1)
    
    # è¿è¡Œå¤šç›®æ ‡äº¤å‰åœºæ™¯æµ‹è¯•
    stats = comparison.run_comparison("multiple_crossing", duration=10.0)
    
    print("\\nâœ… åˆ†æå®Œæˆï¼")
    print("ğŸ“ˆ å›¾è¡¨æ–‡ä»¶: tracking_configuration_comparison.png")
    print("ğŸ” è¿™ä¸ªåˆ†æå±•ç¤ºäº†æ ‡å‡†é…ç½®å’Œé«˜ç²¾åº¦é…ç½®è·Ÿè¸ªå™¨çš„æ€§èƒ½å·®å¼‚")

if __name__ == "__main__":
    main()
